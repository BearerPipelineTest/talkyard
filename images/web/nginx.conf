# Nginx config file.

# To see Nginx build options:
#   2>&1 nginx -V | tr ' ' '\n'

# Optimizing Nginx:
#  http://engineering.chartbeat.com/2014/01/02/part-1-lessons-learned-tuning-tcp-and-nginx-in-ec2/


user nginx;

# 'auto' attempts to auto detect the number of CPU cores.
# For now, use 1 though, because if there's just one worker, and it crashes,
# Nginx auto restarts it. [NGXSEGFBUG]
# COULD_OPTIMIZE change to 'auto' — but that's not important; Nginx is not
# a bottleneck anyway. (Buffering the Nginx access_log is more important?)
# Maybe rethink  worker_connections  below, if changing this.
#
worker_processes 1;  # later: 'auto'

# Core dump path, and max core dump size. They can be about 200M large (so 400M usually large enough?).
#working_directory /tmp/cores/;  # needs to mount/create dir, otherwise Nginx won't start [NGXCORED]
#worker_rlimit_core 400M;

# Run nginx in the foreground.
# (We run nginx via Docker, and by default the nginx command exits directly (because it
# starts nginx as a background daemon) which makes the docker container exits, and then
# the background daemon stops too.)
daemon off;

events {
  # In the Docker image, `ulimit -n` –> 524288, and say 10 cores? -> max 52400, say 30 000 then
  # Currently just 1 worker process though (see above) so let's pick 50 000 for now.
  # Max total connections = worker_processes * worker_connections.
  worker_connections 50000;
}


# Enable just-in-time compilation of regular expressions in the Nginx config
# files, at startup. "Can speed up processing of regular expressions significantly",
# http://nginx.org/en/docs/ngx_core_module.html#pcre_jit.
# Apparently by default Nginx otherwise evaluates regexs dynamically after startup:
# https://www.getpagespeed.com/server-setup/nginx-locations-performance-impact-and-optimizations
#
# This needs package pcre-devel and build flag --with-pcre-jit.
# (can test: pcretest -C   that needs:  apk add pcre   not just  pcre-dev  ?)
# (Seems PCRE isn't used at all in Nginx official images:
# https://github.com/nginxinc/docker-nginx/issues/147#issuecomment-279330224
# > As we're not building PCRE in the official images [pcre_jit on will not help].
# But then how does Nginx by default handle location regexs?
# Seems it just doesn't work, by default:
# https://www.computers.wtf/posts/nginx-with-pcre/
# > [ if you use a location regex or rewrite ] you’ll get a 404 Not Found response
# > from Nginx. That’s because PCRE is required for regular expressions’ support
# > in the location directive and for ngx_http_rewrite_module to kick in,
# > but Nginx doesn’t come integrated with it out of the box.  )
#
# Tested with:
#   location ~ ^/-/(funny-bunny|regex-ping|zip-zap(.v0)?).(?<suffix>min.js|js)$ {
#     return 200 'regex-pong';
#     add_header Content-Type text/plain;
#   }
# And:  wrk -t 4 -c 60 -d 10   http://localhost/-/regex-ping.min.js
# and access and error log off. Didn't seem to make any difference
# — about 31k req/s in the Docker container in a VM in my core i5 laptop
# both with 'pcre_jit on' and without any 'pcre_jit ...'  (Dec '20).
#
pcre_jit on;



http {
  include  /etc/nginx/mime.types;
  default_type text/plain;

  # Specify charset, so utf-8 symbols in js and css files look ok. And add 'text/css'
  # to the content types for which the charset is specified (not incl by default).
  charset utf-8;
  # text/html is included by default — listing it below, too, would cause a
  #   "duplicate MIME type "text/html" in /etc/nginx/nginx.conf"
  # warning.
  charset_types text/xml text/plain text/vnd.wap.wml application/javascript application/rss+xml text/css ;
  source_charset utf-8;

  server_tokens off;

  # Lua needs a resolver, otherwise e.g. lua-resty-http won't find 'app'.
  # `local=on` makes us use /etc/resolv.conf — there, Docker has
  # added a nameserver. For now, let's use 127.0.0.11 — Docker
  # has a nameserver there, see:
  #  https://stackoverflow.com/a/37656784  and
  #  https://github.com/moby/moby/issues/22652#issuecomment-218413598 )
  resolver 127.0.0.11 ipv6=off; # local=on — needs OpenResty
  resolver_timeout 8s;

  # Prevent clickjacking. Now moved to Scala instead [7ACKRQ20]
  # allow_embedding_from, and use 'Content-Security-Policy: frame-ancestors ...' instead, for Chrome.
  #add_header X-Frame-Options DENY;

  sendfile on;
  sendfile_max_chunk 2m;
  tcp_nopush on; # if off, then like 14 100 req/sec instead of like 15 300, for 1kb image.

  # We don't require so very lower latency on every packet sent.
  # And John Nagle says it's not needed? https://news.ycombinator.com/item?id=9045125
  tcp_nodelay off;

  # Asynchronous file I/O, 'aio threads' offloads read and send file operations
  # to threads of the specified pool.
  # This makes things slower, as of some time long ago:
  # 14 600 req/sec instead of like 15 300, for 1kb image.
  #
  # aio threads;  # or threads=poolname, default name is 'default'.
  #
  # Not needed?: thread_pool  — there's a 'default' thread pool.
  # There's also:  directio on/off.

  keepalive_timeout  65;

  include  /etc/nginx/http-limits.conf;


  # ----- Lua

  # For development edit-refresh without reloading the Nginx config:
  #lua_code_cache off;

  #lua_package_path '/opt/lua-packages/lua-resty-lrucache/lib/?.lua;/opt/lua-packages/lua-resty-redis/lib/?.lua;;';
  #lua_package_path '/opt/lua-packages/lua-resty-http/lib/?.lua;/opt/talkyard/lua/lua-limit-bandwidth/?.lua;;';
  lua_package_path '/opt/lua-packages/?.lua;lua-packages/inspect.lua/?.lua;/opt/talkyard/lua/?.lua;;';
  include /opt/talkyard/lua/lua-limit-bandwidth/lua-limit-bandwidth.conf;


  # ----- Lua plugins

  # It's nice to have everything here directly in nginx.conf, although
  # the recommendation is to use  ..._by_lua_file instead. But that'd mean
  # all different ..._by_lua... got split into different files, making it
  # a bit time consuming to get an overview over all Lua stuff.

  # The plugins:
  # - lua-limit-bandwidth: Restrict outgoing bandwidth, to try to avoid getting any
  #   crazily expensive cloud hosting surprise invoice.
  # - lua-resty-acme: Auto HTTPS via Lua plugin

  lua_shared_dict acme 16m;

  # This needed 1) to verify that the certs we'll get from LetsEncrypt are valid?
  # Or 2) for the browser to trust our LetsEncrypt issued certs?
  lua_ssl_trusted_certificate /etc/ssl/certs/ca-certificates.crt;
  lua_ssl_verify_depth 2;

  init_by_lua_block {
    -- Bandwidth limiting
    -- This loads: /opt/talkyard/lua/lua-limit-bandwidth/init.lua
    -- which is (in the dev repo): ./ed-lua/lua-limit-bandwidth/init.lua
    require("lua-limit-bandwidth/init")

    -- Auto HTTPS
    -- Find default config here:
    -- ./openresty-pkgs/usr-local-openresty-site/lualib/resty/acme/autossl.lua
    --
    require("resty.acme.autossl").init({
      -- If one has accepted the LetsEncrypt ToS, https://letsencrypt.org/repository/.
      -- SHOULD be configurable so self-hosted admins need to edit & change to true?
      tos_accepted = true,

      -- Set to true in test env, to use LetsEncrypt staging env, and avoid
      -- "Too many requests" errors when using LetsEncrypt for real.
      -- staging = true,

      -- By default only RSA (not ECC) enabled.
      -- domain_key_types = { 'rsa', 'ecc' },

      -- By default only challenges over HTTP (not HTTPS, tls-alpn-01) enabled.
      -- In lua-resty-acme, tls-alpn-01 is experimental. So not incl here.
      -- If enabling, remember to also update:
      --    conf/sites-enabled-manual/talkyard-servers.conf,
      -- enabled_challenge_handlers = { 'http-01', 'tls-alpn-01' },

      -- LetsEncrypt's rate limits are higher, if one specifies an account key.
      account_key_path = "/etc/nginx/acme-account.key",

      -- SHOULD be configurable, default empty.
      account_email = "security@talkyard.io",

      -- For now, when testing.
      domain_whitelist = {
          'autohttps.localhost', 'autohttps2.localhost',
          'autohttps.talkyard.io', 'autohttps2.talkyard.io', 'autohttps3.talkyard.io',
          'sub.autos.talkyard.io', 'sub.autos2.talkyard.io' },
    })
    -- Reset VSCode highlighting: :-P  '
  }

  # The worker will share the 'local var_name' variables inited in init_by_lua_block{}
  # above — read more here:
  # https://github.com/openresty/lua-nginx-module#data-sharing-within-an-nginx-worker
  # to learn about how data sharing in Nginx workers and Lua works.
  # (Once inited, those local variables aren't changed — only read.)
  init_worker_by_lua_block {
    ngx.log(ngx.INFO, 'Running: init_worker_by_lua_block')
    require("resty.acme.autossl").init_worker()
  }

  access_by_lua_block {
     -- This loads: /opt/talkyard/lua/lua-limit-bandwidth/access-phase.lua
    -- which is (in the dev repo): ./ed-lua/lua-limit-bandwidth/access-phase.lua
     require("lua-limit-bandwidth/access-phase")
  }

  log_by_lua_block {
    -- This loads: /opt/talkyard/lua/lua-limit-bandwidth/log-phase.lua
    -- which is (in the dev repo): ./ed-lua/lua-limit-bandwidth/log-phase.lua
    require("lua-limit-bandwidth/log-phase")
  }


  # ----- HTTPS session cache

  # (HTTPS hardening is instead done in ssl-hardening.conf, because should be placed
  # directly after the cert and key directives.)

  # Avoid CPU intensive SSL handshake. 1MB cache contains about 4000 sessions.
  # The default timeout is 5 minutes
  # (See http://nginx.org/en/docs/http/configuring_https_servers.html)
  ssl_session_cache shared:SSL:15m;  # > 15m for large servers?  [memory]
  ssl_session_timeout 10m;


  # ----- Logging

  log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

  # COULD_OPTIMIZE  buffer= ...
  access_log  /var/log/nginx/access.log  main; #  later:  buffer=64K  flush=5m;

  error_log /var/log/nginx/error.log ${TY_NGX_ERR_LOG_LEVEL};
  error_page 502 /502.html;
  error_page 503 /503.html;


  # ----- Reverse proxy cache

  proxy_cache_path /opt/nginx/proxy-cache levels=1:2 keys_zone=EfDiCache:8m max_size=1000m inactive=600m;
  proxy_temp_path /opt/nginx/proxy-temp;


  # ----- Virtual hosts

  include /etc/nginx/sites-enabled-manual/*.conf;
  # CLEAN_UP remove default-site after I've moved everything to conf/web/sites-enabled-manual/. [ty_v1]
  include /etc/nginx/default-site/*.conf;
  include /etc/nginx/sites-enabled-auto-gen/*.conf;
  # CLEAN_UP remove sites-enabled after I've moved everything to data/web/sites-enabled-auto-gen/.
  # No, keep? Now when HTTPS will be via lua-resty-auto-ssl, won't need any auto gen sites?
  include /etc/nginx/sites-enabled/*.conf;


  # ----- Bad hostnames

  # Bots on the Internet send requests with weird hostnames. Block them, [block_bad_reqs]
  # otherwise Play Framework logs annoying parser error warnings.
  #
  # Examples:
  #  """Illegal 'host' header: Invalid input '^',
  #     expected 'EOI', ':', UPPER_ALPHA, lower-reg-name-char or pct-encoded:
  #     ~^(.*)\\.talkyard\\.net$\n ^"""
  #  """Request is missing required `Host` header"""

  server {
    listen 80;
    listen 443 ssl;
    server_name ~[`\'\"\\\ \n\t<>()\{\}\[\],\;+*^\$!?&=#];

    # This magic status code (444) makes Nginx abort the TCP connection before
    # the browser shows any certificate error.
    #
    # With HTTPS, Chrome says:
    # > This site can’t be reached
    # > [...] https://the.weird.hostname.com might be temporarily down or
    # > it may have moved permanently to a new web address.
    # > ERR_HTTP2_PROTOCOL_ERROR
    #
    # With HTTP, Chrome says:
    # > This page isn’t working
    # > weird.hostname.com didn’t send any data.
    # > ERR_EMPTY_RESPONSE
    #
    return 444;

    # Not in use; we never reply, see above — this just makes Nginx happy.
    ssl_certificate     /etc/nginx/ssl-cert-snakeoil.pem;
    ssl_certificate_key /etc/nginx/ssl-cert-snakeoil.key;
    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;
  }

}

# vim: et ts=2 sw=2 tw=0 list
